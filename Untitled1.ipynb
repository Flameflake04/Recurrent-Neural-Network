{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2995e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from UIC mapping file, and get 2 columns: UICID and Phase, mapping it into dictionary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Reading the file, store it in a variable as \"dataset_prep_1\"\n",
    "# Command used: pd.read_csv\n",
    "dataset_prep_1 = pd.read_csv(\"E:\\\\Recurrent Neural Network Workflow\\\\UICmapping.csv\",encoding = 'latin1')\n",
    "\n",
    "\n",
    "# Getting the 2 column by name\n",
    "# Command used: df[\"column_name\"]\n",
    "UIC_ID_col = dataset_prep_1[\"#SampleID\"]\n",
    "UIC_Phase_col = dataset_prep_1[\"Phase\"]\n",
    "\n",
    "# Using the zip command to map 2 of the column into a \"mapping_dict\"\n",
    "# Command used: dict(zip(column_1, column_2))\n",
    "mapping_dict = dict(zip(UIC_ID_col, UIC_Phase_col))\n",
    "\n",
    "# Print out and check out the result:\n",
    "print(UIC_ID_col)\n",
    "print(UIC_Phase_col)\n",
    "print(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b614985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the data of the mapping dict, transfer the data into the row title of the second set of data\n",
    "# Converting all UIC_ID into seperate phase and sort it by index, output the new csv file\n",
    "\n",
    "# Reading file 2 and store it as dataset_prep_2\n",
    "dataset_prep_2 = pd.read_csv(\"E:\\\\Recurrent Neural Network Workflow\\\\BIOMtable.csv\",encoding = 'latin1')\n",
    "\n",
    "# Create a copy of dataframe dataset_prep_2\n",
    "# Command used: dataframe.copy()\n",
    "df_copy = dataset_prep_2.copy()\n",
    "\n",
    "\n",
    "# Renaming the dataframe columns, mapping column UIC_ID to corresponding phase in column Phase\n",
    "# Command used: dataframe.rename()\n",
    "df_copy.rename(columns=mapping_dict, inplace=True)\n",
    "\n",
    "# Sorting dataframe by Phase\n",
    "# Command used: dataframe.sort_index(axis = 1)\n",
    "df_copy = df_copy.sort_index(axis=1)\n",
    "\n",
    "# Moving the column has microbiome name into the beginning of the dataframe after sorting\n",
    "# Using pop to delete the column, then using insert to re-insert the column into the start\n",
    "# Command used: dataframe.pop(column_to_move), dataframe.insert(column_to_add)\n",
    "column_to_move = 'Unnamed: 0'\n",
    "new_position = 0 \n",
    "column_index = df_copy.pop(column_to_move)\n",
    "df_copy.insert(new_position, column_to_move, column_index)\n",
    "\n",
    "# Delete all the UIC_ID column that doesnt have phase mapping to it\n",
    "string_to_exclude = 'UIC'\n",
    "df_filtered = df_copy.drop(columns=[col for col in df_copy.columns if string_to_exclude in col])\n",
    "\n",
    "# Create a variable that calculate the sum of row in the dataframe\n",
    "taxa_sums = df_filtered.sum(axis=1)\n",
    "\n",
    "# Edit the dataframe, removing all single OTS\n",
    "df_filtered = df_filtered[taxa_sums > 1]\n",
    "\n",
    "# Exporting file into a new csv, not including the index\n",
    "# Command used: dataframe.to_csv('name.csv', index = False)\n",
    "df_filtered.to_csv('New_mapping_file.csv', index=False)\n",
    "\n",
    "# Print out the dataframe:\n",
    "# print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36def577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set seed for numpy and tensorflow for reproduction\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "dataset_train = pd.read_csv(\"E:\\\\Recurrent Neural Network Workflow\\\\New_mapping_file.csv\")\n",
    "df = dataset_train.copy()\n",
    "#print(df)\n",
    "\n",
    "# Getting total of each Phase for easier training:\n",
    "# Total phase 1: \n",
    "df['Phase 1 Total'] = df.iloc[:, 1:10].sum(axis=1)\n",
    "# Total phase 2:\n",
    "df['Phase 2 Total'] = df.iloc[:, 10:50].sum(axis=1)\n",
    "# Total phase 3:\n",
    "df['Phase 3 Total'] = df.iloc[:, 50:91].sum(axis=1)\n",
    "# Total phase 4:\n",
    "df['Phase 4 Total'] = df.iloc[:,91:105].sum(axis=1)\n",
    "# Total phase 5:\n",
    "df['Phase 5 Total'] = df.iloc[:,105:119].sum(axis=1)\n",
    "# Total phase 6:\n",
    "df['Phase 6 Total'] = df.iloc[:,119:133].sum(axis=1)\n",
    "\n",
    "# Getting all phase into one new dataframe and output\n",
    "new_df = pd.concat([df['Phase 1 Total'] , df['Phase 2 Total'] , df['Phase 3 Total'] , df['Phase 4 Total'] , df['Phase 5 Total'], df['Phase 6 Total']], axis=1)\n",
    "#print(new_df)\n",
    "\n",
    "# Getting 70% of data for training, output random sample size, as well as set seed\n",
    "sample_size = int(0.7 * len(new_df))\n",
    "sampled_data = new_df.sample(n=sample_size, random_state = 123)\n",
    "print(sampled_data)\n",
    "\n",
    "# Convert data into numpy\n",
    "data = new_df.values\n",
    "num_otus, num_phases = data.shape\n",
    "\n",
    "time_steps = 2\n",
    "num_features = data.shape[1]\n",
    "\n",
    "# Prepare data for RNN\n",
    "x, y = [], []\n",
    "\n",
    "for i in range(len(data) - time_steps):\n",
    "    x.append(data[i:i+time_steps])\n",
    "    y.append(data[i+time_steps])\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=64, input_shape=(time_steps, num_features)))\n",
    "model.add(Dense(units=num_features))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x, y, epochs=100, batch_size= 32)  \n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(x)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf687bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
