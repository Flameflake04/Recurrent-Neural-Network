{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1b8f61",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      UIC01\n",
      "1      UIC02\n",
      "2      UIC03\n",
      "3      UIC04\n",
      "4      UIC05\n",
      "       ...  \n",
      "127    UIC95\n",
      "128    UIC96\n",
      "129    UIC97\n",
      "130    UIC98\n",
      "131    UIC99\n",
      "Name: #SampleID, Length: 132, dtype: object\n",
      "0      Phase.1\n",
      "1      Phase.1\n",
      "2      Phase.1\n",
      "3      Phase.1\n",
      "4      Phase.1\n",
      "        ...   \n",
      "127    Phase.2\n",
      "128    Phase.3\n",
      "129    Phase.3\n",
      "130    Phase.4\n",
      "131    Phase.4\n",
      "Name: Phase, Length: 132, dtype: object\n",
      "{'UIC01': 'Phase.1', 'UIC02': 'Phase.1', 'UIC03': 'Phase.1', 'UIC04': 'Phase.1', 'UIC05': 'Phase.1', 'UIC06': 'Phase.1', 'UIC07': 'Phase.1', 'UIC08': 'Phase.1', 'UIC09': 'Phase.1', 'UIC10': 'Phase.2', 'UIC100': 'Phase.5', 'UIC101': 'Phase.5', 'UIC102': 'Phase.2', 'UIC103': 'Phase.2', 'UIC104': 'Phase.3', 'UIC105': 'Phase.3', 'UIC106': 'Phase.4', 'UIC107': 'Phase.4', 'UIC108': 'Phase.5', 'UIC109': 'Phase.5', 'UIC11': 'Phase.2', 'UIC110': 'Phase.2', 'UIC111': 'Phase.2', 'UIC112': 'Phase.3', 'UIC113': 'Phase.3', 'UIC114': 'Phase.4', 'UIC115': 'Phase.4', 'UIC116': 'Phase.5', 'UIC117': 'Phase.5', 'UIC118': 'Phase.2', 'UIC119': 'Phase.2', 'UIC12': 'Phase.2', 'UIC120': 'Phase.3', 'UIC121': 'Phase.3', 'UIC122': 'Phase.4', 'UIC123': 'Phase.4', 'UIC124': 'Phase.5', 'UIC125': 'Phase.5', 'UIC13': 'Phase.2', 'UIC14': 'Phase.2', 'UIC15': 'Phase.2', 'UIC16': 'Phase.2', 'UIC166': 'Phase.6', 'UIC167': 'Phase.6', 'UIC168': 'Phase.6', 'UIC169': 'Phase.6', 'UIC17': 'Phase.2', 'UIC170': 'Phase.6', 'UIC171': 'Phase.6', 'UIC172': 'Phase.6', 'UIC173': 'Phase.6', 'UIC174': 'Phase.6', 'UIC175': 'Phase.6', 'UIC176': 'Phase.6', 'UIC177': 'Phase.6', 'UIC178': 'Phase.6', 'UIC179': 'Phase.6', 'UIC18': 'Phase.2', 'UIC19': 'Phase.2', 'UIC20': 'Phase.2', 'UIC21': 'Phase.2', 'UIC22': 'Phase.2', 'UIC23': 'Phase.2', 'UIC24': 'Phase.2', 'UIC25': 'Phase.2', 'UIC26': 'Phase.2', 'UIC27': 'Phase.2', 'UIC29': 'Phase.2', 'UIC30': 'Phase.2', 'UIC31': 'Phase.2', 'UIC32': 'Phase.2', 'UIC33': 'Phase.2', 'UIC34': 'Phase.2', 'UIC35': 'Phase.2', 'UIC36': 'Phase.2', 'UIC37': 'Phase.3', 'UIC38': 'Phase.3', 'UIC39': 'Phase.3', 'UIC40': 'Phase.3', 'UIC41': 'Phase.3', 'UIC42': 'Phase.3', 'UIC43': 'Phase.3', 'UIC44': 'Phase.3', 'UIC45': 'Phase.3', 'UIC46': 'Phase.3', 'UIC47': 'Phase.3', 'UIC48': 'Phase.3', 'UIC49': 'Phase.3', 'UIC50': 'Phase.3', 'UIC51': 'Phase.3', 'UIC52': 'Phase.3', 'UIC53': 'Phase.3', 'UIC54': 'Phase.3', 'UIC55': 'Phase.3', 'UIC56': 'Phase.3', 'UIC57': 'Phase.3', 'UIC58': 'Phase.3', 'UIC59': 'Phase.3', 'UIC60': 'Phase.3', 'UIC61': 'Phase.3', 'UIC62': 'Phase.3', 'UIC63': 'Phase.3', 'UIC70': 'Phase.2', 'UIC71': 'Phase.2', 'UIC72': 'Phase.3', 'UIC73': 'Phase.3', 'UIC74': 'Phase.4', 'UIC75': 'Phase.4', 'UIC76': 'Phase.5', 'UIC77': 'Phase.5', 'UIC78': 'Phase.2', 'UIC79': 'Phase.2', 'UIC80': 'Phase.3', 'UIC81': 'Phase.3', 'UIC82': 'Phase.4', 'UIC83': 'Phase.4', 'UIC84': 'Phase.5', 'UIC85': 'Phase.5', 'UIC86': 'Phase.2', 'UIC87': 'Phase.2', 'UIC88': 'Phase.3', 'UIC89': 'Phase.3', 'UIC90': 'Phase.4', 'UIC91': 'Phase.4', 'UIC92': 'Phase.5', 'UIC93': 'Phase.5', 'UIC94': 'Phase.2', 'UIC95': 'Phase.2', 'UIC96': 'Phase.3', 'UIC97': 'Phase.3', 'UIC98': 'Phase.4', 'UIC99': 'Phase.4'}\n"
     ]
    }
   ],
   "source": [
    "# Read the data from UIC mapping file, and get 2 columns: UICID and Phase, mapping it into dictionary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Reading the file, store it in a variable as \"dataset_prep_1\"\n",
    "# Command used: pd.read_csv\n",
    "dataset_prep_1 = pd.read_csv(\"E:\\\\Recurrent Neural Network Workflow\\\\UICmapping.csv\",encoding = 'latin1')\n",
    "\n",
    "\n",
    "# Getting the 2 column by name\n",
    "# Command used: df[\"column_name\"]\n",
    "UIC_ID_col = dataset_prep_1[\"#SampleID\"]\n",
    "UIC_Phase_col = dataset_prep_1[\"Phase\"]\n",
    "\n",
    "# Using the zip command to map 2 of the column into a \"mapping_dict\"\n",
    "# Command used: dict(zip(column_1, column_2))\n",
    "mapping_dict = dict(zip(UIC_ID_col, UIC_Phase_col))\n",
    "\n",
    "# Print out and check out the result:\n",
    "print(UIC_ID_col)\n",
    "print(UIC_Phase_col)\n",
    "print(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da21a359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Using the data of the mapping dict, transfer the data into the row title of the second set of data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Converting all UIC_ID into seperate phase and sort it by index, output the new csv file\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Reading file 2 and store it as dataset_prep_2\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m dataset_prep_2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mRecurrent Neural Network Workflow\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mBIOMtable.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create a copy of dataframe dataset_prep_2\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Command used: dataframe.copy()\u001b[39;00m\n\u001b[0;32m      9\u001b[0m df_copy \u001b[38;5;241m=\u001b[39m dataset_prep_2\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Using the data of the mapping dict, transfer the data into the row title of the second set of data\n",
    "# Converting all UIC_ID into seperate phase and sort it by index, output the new csv file\n",
    "\n",
    "# Reading file 2 and store it as dataset_prep_2\n",
    "dataset_prep_2 = pd.read_csv(\"E:\\\\Recurrent Neural Network Workflow\\\\BIOMtable.csv\",encoding = 'latin1')\n",
    "\n",
    "# Create a copy of dataframe dataset_prep_2\n",
    "# Command used: dataframe.copy()\n",
    "df_copy = dataset_prep_2.copy()\n",
    "\n",
    "\n",
    "# Renaming the dataframe columns, mapping column UIC_ID to corresponding phase in column Phase\n",
    "# Command used: dataframe.rename()\n",
    "df_copy.rename(columns=mapping_dict, inplace=True)\n",
    "\n",
    "# Sorting dataframe by Phase\n",
    "# Command used: dataframe.sort_index(axis = 1)\n",
    "df_copy = df_copy.sort_index(axis=1)\n",
    "\n",
    "# Moving the column has microbiome name into the beginning of the dataframe after sorting\n",
    "# Using pop to delete the column, then using insert to re-insert the column into the start\n",
    "# Command used: dataframe.pop(column_to_move), dataframe.insert(column_to_add)\n",
    "column_to_move = 'Unnamed: 0'\n",
    "new_position = 0 \n",
    "column_index = df_copy.pop(column_to_move)\n",
    "df_copy.insert(new_position, column_to_move, column_index)\n",
    "\n",
    "# Delete all the UIC_ID column that doesnt have phase mapping to it\n",
    "string_to_exclude = 'UIC'\n",
    "df_filtered = df_copy.drop(columns=[col for col in df_copy.columns if string_to_exclude in col])\n",
    "\n",
    "# Create a variable that calculate the sum of row in the dataframe\n",
    "taxa_sums = df_filtered.sum(axis=1)\n",
    "\n",
    "# Edit the dataframe, removing all single OTS\n",
    "df_filtered = df_filtered[taxa_sums > 1]\n",
    "\n",
    "# Exporting file into a new csv, not including the index\n",
    "# Command used: dataframe.to_csv('name.csv', index = False)\n",
    "df_filtered.to_csv('New_mapping_file.csv', index=False)\n",
    "\n",
    "# Print out the dataframe:\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b7b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attirbute of the data:\n",
    "# 1. Uneven number of columns for timeframe\n",
    "# 2. Train data with multiple column of timeframe\n",
    "# 3. Phase 1, phase 2, phase 3 is in season, phase 4 is competition, phase 5 and phase 6 is off-season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba5e6016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0      1      2      3      4      5      6      7      8      \\\n",
      "Phase 1 Total      0     44      0      6      0      0      0      0      0   \n",
      "Phase 2 Total      0    338      3      4      0      1      1      0      2   \n",
      "Phase 3 Total      0    428      0     21      0      1      0      2      3   \n",
      "Phase 4 Total      0     98      1      0      0      0      0      1      0   \n",
      "Phase 5 Total      0    155      1      0      0      0      0      2      0   \n",
      "Phase 6 Total      3     99      0      0      3      0      3      1      0   \n",
      "\n",
      "               9      ...  12296  12297  12298  12299  12300  12301  12302  \\\n",
      "Phase 1 Total      0  ...    606   1156   1660   1206     19   5848   1185   \n",
      "Phase 2 Total      3  ...    623  13774  11821  10929   1206  40513  15318   \n",
      "Phase 3 Total      0  ...    804  10018   8995   7373    280  36422   6726   \n",
      "Phase 4 Total      5  ...      0   8802   7675   8673    747  22942   9995   \n",
      "Phase 5 Total      1  ...      0  16150   5109  11428     77  35858   5747   \n",
      "Phase 6 Total      0  ...      0  15357   4806   9768    678  46572   9008   \n",
      "\n",
      "               12303  12304  12305  \n",
      "Phase 1 Total     21    898    162  \n",
      "Phase 2 Total   3167   3666   7798  \n",
      "Phase 3 Total    185   1037   1919  \n",
      "Phase 4 Total     25      0   5897  \n",
      "Phase 5 Total      5      0    756  \n",
      "Phase 6 Total    158      0   4594  \n",
      "\n",
      "[6 rows x 12306 columns]\n",
      "               0      1      2      3      4      5      6      7      8      \\\n",
      "Phase 2 Total      0    338      3      4      0      1      1      0      2   \n",
      "Phase 4 Total      0     98      1      0      0      0      0      1      0   \n",
      "Phase 5 Total      0    155      1      0      0      0      0      2      0   \n",
      "Phase 1 Total      0     44      0      6      0      0      0      0      0   \n",
      "\n",
      "               9      ...  12296  12297  12298  12299  12300  12301  12302  \\\n",
      "Phase 2 Total      3  ...    623  13774  11821  10929   1206  40513  15318   \n",
      "Phase 4 Total      5  ...      0   8802   7675   8673    747  22942   9995   \n",
      "Phase 5 Total      1  ...      0  16150   5109  11428     77  35858   5747   \n",
      "Phase 1 Total      0  ...    606   1156   1660   1206     19   5848   1185   \n",
      "\n",
      "               12303  12304  12305  \n",
      "Phase 2 Total   3167   3666   7798  \n",
      "Phase 4 Total     25      0   5897  \n",
      "Phase 5 Total      5      0    756  \n",
      "Phase 1 Total     21    898    162  \n",
      "\n",
      "[4 rows x 12306 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tran Xuan Duc\\AppData\\Local\\Temp\\ipykernel_7832\\961401563.py:12: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset_train = pd.read_csv(\"E:\\\\Recurrent Neural Network Workflow\\\\New_mapping_file.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Convert data into numpy\\ndata = new_df.values\\nnum_otus, num_phases = data.shape\\n\\ntime_steps = 2\\nnum_features = data.shape[1]\\n\\n# Prepare data for RNN\\nx, y = [], []\\n\\nfor i in range(len(data) - time_steps):\\n    x.append(data[i:i+time_steps])\\n    y.append(data[i+time_steps])\\n\\nx = np.array(x)\\ny = np.array(y)\\n\\n# Define the RNN model\\nmodel = Sequential()\\nmodel.add(LSTM(units=64, input_shape=(time_steps, num_features)))\\nmodel.add(Dense(units=num_features))\\n\\n# Compile the model\\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mean_absolute_error\"])\\n\\n# Train the model\\nhistory = model.fit(x, y, epochs=100, batch_size= 32)  \\n\\n# Make predictions\\npredictions = model.predict(x)\\n\\n# Print the predictions\\nprint(predictions)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set seed for numpy and tensorflow for reproduction\n",
    "np.random.seed(8)\n",
    "tf.random.set_seed(8)\n",
    "\n",
    "dataset_train = pd.read_csv(\"E:\\\\Recurrent Neural Network Workflow\\\\New_mapping_file.csv\")\n",
    "df = dataset_train.copy()\n",
    "#print(df)\n",
    "\n",
    "# Getting total of each Phase for easier training:\n",
    "# Total phase 1: \n",
    "df['Phase 1 Total'] = df.iloc[:, 1:10].sum(axis=1)\n",
    "# Total phase 2:\n",
    "df['Phase 2 Total'] = df.iloc[:, 10:50].sum(axis=1)\n",
    "# Total phase 3:\n",
    "df['Phase 3 Total'] = df.iloc[:, 50:91].sum(axis=1)\n",
    "# Total phase 4:\n",
    "df['Phase 4 Total'] = df.iloc[:,91:105].sum(axis=1)\n",
    "# Total phase 5:\n",
    "df['Phase 5 Total'] = df.iloc[:,105:119].sum(axis=1)\n",
    "# Total phase 6:\n",
    "df['Phase 6 Total'] = df.iloc[:,119:133].sum(axis=1)\n",
    "\n",
    "# Getting all phase into one new dataframe and output\n",
    "new_df_need_to_transpose = pd.concat([df['Phase 1 Total'] , df['Phase 2 Total'] , df['Phase 3 Total'] , df['Phase 4 Total'] , df['Phase 5 Total'], df['Phase 6 Total']], axis=1)\n",
    "new_df = new_df_need_to_transpose.transpose()\n",
    "print(new_df)\n",
    "\n",
    "# Getting 70% of data for training, output random sample size, as well as set seed\n",
    "sample_size = int(0.7 * len(new_df))\n",
    "sampled_data = new_df.sample(n=sample_size, random_state = 123)\n",
    "print(sampled_data)\n",
    "'''\n",
    "# Convert data into numpy\n",
    "data = new_df.values\n",
    "num_otus, num_phases = data.shape\n",
    "\n",
    "time_steps = 2\n",
    "num_features = data.shape[1]\n",
    "\n",
    "# Prepare data for RNN\n",
    "x, y = [], []\n",
    "\n",
    "for i in range(len(data) - time_steps):\n",
    "    x.append(data[i:i+time_steps])\n",
    "    y.append(data[i+time_steps])\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=64, input_shape=(time_steps, num_features)))\n",
    "model.add(Dense(units=num_features))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x, y, epochs=100, batch_size= 32)  \n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(x)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a94f553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82120e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5e18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d4d4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
